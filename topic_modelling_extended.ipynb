{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @author: giuseppec\n",
    "\n",
    "###### credits to Selva Prabhakaran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "#nltk.download('stopwords')\n",
    "#!python -m spacy download en\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herse\"', 'him', 'himse\"', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itse\"', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myse\"', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "\n",
    "\n",
    "#from IPython import get_ipython\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(968, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRED</td>\n",
       "      <td>while security forces are more likely to be prediction and physical measures oriented.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRED</td>\n",
       "      <td>focused on planning and prediction practices in preparation for future terrorist attacks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IDEN</td>\n",
       "      <td>identifying potential targets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRED</td>\n",
       "      <td>preparing a recovery plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRED</td>\n",
       "      <td>developing statistical forecasting systems and destination specific antiterrorism action plans.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code  \\\n",
       "0  PRED   \n",
       "1  PRED   \n",
       "2  IDEN   \n",
       "3  PRED   \n",
       "4  PRED   \n",
       "\n",
       "                                                                                               Text  \n",
       "0           while security forces are more likely to be prediction and physical measures oriented.   \n",
       "1        focused on planning and prediction practices in preparation for future terrorist attacks.   \n",
       "2                                                                    identifying potential targets   \n",
       "3                                                                        preparing a recovery plan   \n",
       "4  developing statistical forecasting systems and destination specific antiterrorism action plans.   "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('CodingOverview_GREY_no_DATA.csv', sep=',', encoding='latin-1', engine='python')\n",
    "#df = pd.read_csv('CodingOverview_no_Data.csv', sep=',', encoding='utf-8', engine='python')\n",
    "df = pd.read_csv('protect_dataset_nodata.csv', sep=',', encoding='utf-8', engine='python')\n",
    "#df = pd.read_csv(\"vulnerabilities_TOT.txt\", delimiter='\\n', encoding='latin-1', engine='python', skip_blank_lines=True)\n",
    "#df = pd.read_csv(\"vulnerabilities_TOT.txt\", sep='\\n', encoding='latin-1', engine='python', skip_blank_lines=True)\n",
    "#df = pd.read_csv('CodingOverview_GREY_no_DATA.csv', sep=',', encoding='latin1', engine='python')\n",
    "\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset for Topological Analysis as a List of List [['a'],['b'],['c'],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_analysis = [] \n",
    "\n",
    "#df2 = pd.read_csv('CodingOverview_GREY_no_DATA.csv', sep=',', encoding='latin-1', engine='python')\n",
    "#df2 = pd.read_csv('CodingOverview_no_Data.csv', sep=',', encoding='latin-1', engine='python')\n",
    "#df2 = pd.read_csv('protect_dataset_nodata.csv', sep=',', encoding='latin-1', engine='python')\n",
    "\n",
    "\n",
    "\n",
    "temp_list = list(df.Text)\n",
    "\n",
    "for el in temp_list: \n",
    "        sub = el.split(', ') \n",
    "        top_analysis.append(sub) \n",
    "\n",
    "\n",
    "print(top_analysis[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distribution of the words in the text that have not been lemmatized yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the emails, new line characters, single quotes. Setting the deacc=True option removes punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        #sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        #sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        #sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        #sent = re.sub(\"\\*\", '', sent)  # remove *\n",
    "        #sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to list  ---> TEXT is the column i want to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list  ---> TEXT is the column i want to convert - KEEP THE PREVIOUS TAB COMMENTED (#sent = re.sub('\\S*@\\, #sent = re.sub('\\s+' )\n",
    "data = df.Text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Bigram, Trigram Models and Lemmatize    \n",
    "### Let’s form the bigram and trigrams using the Phrases model. This is passed to Phraser() for efficiency in speed of execution.\n",
    "### Next, lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.\n",
    "### We keep only these POS tags because they are the ones contributing the most to the meaning of the sentences. Here, I use spacy for lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "#!python3 -m spacy download en  # run in terminal once\n",
    "#def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']): ### Too hard as a lemmetisations\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ']): ### softer as lemmatisation\n",
    "\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Topic Model\n",
    "## To build the LDA topic model using LdaModel(), you need the corpus and the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.043*\"security\" + 0.025*\"different\" + 0.017*\"behavior\" + 0.015*\"approach\" '\n",
      "  '+ 0.014*\"bollard\" + 0.013*\"human\" + 0.013*\"service\" + 0.012*\"location\" + '\n",
      "  '0.011*\"measure\" + 0.010*\"point\"'),\n",
      " (1,\n",
      "  '0.018*\"crime\" + 0.015*\"city\" + 0.015*\"new\" + 0.014*\"analysis\" + '\n",
      "  '0.014*\"human\" + 0.014*\"model\" + 0.013*\"terrorist\" + 0.012*\"attack\" + '\n",
      "  '0.011*\"police\" + 0.011*\"pattern\"'),\n",
      " (2,\n",
      "  '0.029*\"detection\" + 0.020*\"target\" + 0.018*\"frame\" + 0.018*\"large\" + '\n",
      "  '0.017*\"solution\" + 0.015*\"model\" + 0.014*\"resource\" + 0.012*\"object\" + '\n",
      "  '0.011*\"value\" + 0.010*\"building\"'),\n",
      " (3,\n",
      "  '0.021*\"vehicle\" + 0.020*\"motion\" + 0.019*\"device\" + 0.017*\"machine\" + '\n",
      "  '0.017*\"time\" + 0.016*\"intelligence\" + 0.015*\"space\" + 0.014*\"high\" + '\n",
      "  '0.013*\"layer\" + 0.012*\"image\"'),\n",
      " (4,\n",
      "  '0.022*\"video\" + 0.015*\"time\" + 0.014*\"event\" + 0.014*\"surveillance\" + '\n",
      "  '0.013*\"real\" + 0.012*\"abnormal\" + 0.012*\"algorithm\" + 0.011*\"camera\" + '\n",
      "  '0.011*\"criminal\" + 0.011*\"people\"'),\n",
      " (5,\n",
      "  '0.032*\"information\" + 0.017*\"edge\" + 0.016*\"technology\" + '\n",
      "  '0.015*\"management\" + 0.015*\"sensor\" + 0.014*\"dynamic\" + 0.013*\"source\" + '\n",
      "  '0.013*\"network\" + 0.012*\"task\" + 0.012*\"people\"')]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=6, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True\n",
    "                                           )\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distribution of sentences and codes among the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = 0\n",
    "\n",
    "#print(\"DEBUG\")\n",
    "#print(doc_term_matrix)\n",
    "\n",
    "\n",
    "number_topics = 2\n",
    "cluster_topics = []\n",
    "for i in range(number_topics):\n",
    "    cluster_topics.append([])\n",
    "\n",
    "###ELSEIF in base al numero di topic che si hanno\n",
    "for doc in  corpus:\n",
    "    vector = lda_model[doc]\n",
    "    #print(vector)\n",
    "    #print()\n",
    "    #print(vector)\n",
    "\n",
    "    index_topic = 0\n",
    "    topics = []\n",
    "    for index_topic in range(len(doc)):\n",
    "        topics.append(vector[1][index_topic])\n",
    "\n",
    "    \n",
    "    topics_prob = lda_model.get_document_topics(doc)\n",
    "    \n",
    "    # Get the topic with the highest probability\n",
    "    max_prob = 0\n",
    "    topic = 0\n",
    "    for topic_prob in topics_prob:\n",
    "        prob = topic_prob[1]\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            topic = topic_prob[0]\n",
    "    \n",
    "    \"\"\"tmp_topics = set()\n",
    "    for word_topics in topics:\n",
    "        for topic in word_topics[1]:\n",
    "            tmp_topics.add(topic)\"\"\"\n",
    "    \n",
    "    \n",
    "    cluster_topics[topic].append((df['Code'][indice],data_words[indice]))####prende il codice in DF\n",
    "        \n",
    "    indice = indice + 1\n",
    "    \n",
    "#####print su file di tutti i topics\n",
    "\n",
    "def stampa(nome_file, cluster):\n",
    "    with open(nome_file+'.csv', 'w') as f:\n",
    "        for item in cluster:\n",
    "            f.write(\"{},{}\\n\".format(item[0], item[1]))\n",
    "\n",
    "for index_cluster in range(len(cluster_topics)):\n",
    "    stampa(\"grey_topic\"+str(index_cluster+1), cluster_topics[index_cluster]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Compute c_v coherence for various number of topics\n",
    "##    Parameters:\n",
    "###    ----------\n",
    "###    dictionary : Gensim dictionary\n",
    "###    corpus : Gensim corpus\n",
    "###    texts : List of input texts\n",
    "###    limit : Max num of topics\n",
    "##    Returns:\n",
    "###    -------\n",
    "###    model_list : List of LDA topic models\n",
    "###    coherence_values : Coherence values corresponding to the LDA model with respective number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = id2word\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=data_ready, start=1, limit=12, step=2)\n",
    "\n",
    "# Show graph\n",
    "limit=12; start=1; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyLDAVis ---- TOPIC MODELLING\n",
    "\n",
    "## pyLDAVis is the most commonly used and a nice way to visualise the information contained in a topic model. \n",
    "## Below is the implementation for LdaModel()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "#pyLDAvis.save_html(vis, 'visualization/topic_modeling/topic_modellingV2.html')\n",
    "pyLDAvis.save_html(vis, 'C:/Users/20185123/Desktop/prova2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Dominant topic and its percentage contribution in each document\n",
    "### In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\n",
    "### This way, you will know which document belongs predominantly to which topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738011</td>\n",
       "      <td>security, different, behavior, approach, bollard, human, service, location, measure, point</td>\n",
       "      <td>[security, force, likely, prediction, physical, measure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907305</td>\n",
       "      <td>crime, city, new, analysis, human, model, terrorist, attack, police, pattern</td>\n",
       "      <td>[focused, planning, prediction, practice, preparation, future, terrorist, attack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.452020</td>\n",
       "      <td>detection, target, frame, large, solution, model, resource, object, value, building</td>\n",
       "      <td>[potential, target]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.389245</td>\n",
       "      <td>information, edge, technology, management, sensor, dynamic, source, network, task, people</td>\n",
       "      <td>[recovery, plan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.383368</td>\n",
       "      <td>information, edge, technology, management, sensor, dynamic, source, network, task, people</td>\n",
       "      <td>[statistical, forecasting, destination, specific, antiterrorism, action, plan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.861046</td>\n",
       "      <td>crime, city, new, analysis, human, model, terrorist, attack, police, pattern</td>\n",
       "      <td>[police, chief, great, importance, prediction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738390</td>\n",
       "      <td>crime, city, new, analysis, human, model, terrorist, attack, police, pattern</td>\n",
       "      <td>[prediction, feasible, terrorist, attack, tourism, destination]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907349</td>\n",
       "      <td>crime, city, new, analysis, human, model, terrorist, attack, police, pattern</td>\n",
       "      <td>[prediction, feasible, category, terrorism, prevention, practice, tourism, director]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.865898</td>\n",
       "      <td>crime, city, new, analysis, human, model, terrorist, attack, police, pattern</td>\n",
       "      <td>[potential, issue, real, time, effective, emergency, man, agement, public, safety, general, quality, life, example, critical, event, earthquake, mob, gathering, protest, spike, social, medium, volume]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.843071</td>\n",
       "      <td>video, time, event, surveillance, real, abnormal, algorithm, camera, criminal, people</td>\n",
       "      <td>[datum, mining, diverse, real, time, social, stream, real, world, event, enable, official, vast, information]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             0.0            0.738011   \n",
       "1            1             1.0            0.907305   \n",
       "2            2             2.0            0.452020   \n",
       "3            3             5.0            0.389245   \n",
       "4            4             5.0            0.383368   \n",
       "5            5             1.0            0.861046   \n",
       "6            6             1.0            0.738390   \n",
       "7            7             1.0            0.907349   \n",
       "8            8             1.0            0.865898   \n",
       "9            9             4.0            0.843071   \n",
       "\n",
       "                                                                                     Keywords  \\\n",
       "0  security, different, behavior, approach, bollard, human, service, location, measure, point   \n",
       "1                crime, city, new, analysis, human, model, terrorist, attack, police, pattern   \n",
       "2         detection, target, frame, large, solution, model, resource, object, value, building   \n",
       "3   information, edge, technology, management, sensor, dynamic, source, network, task, people   \n",
       "4   information, edge, technology, management, sensor, dynamic, source, network, task, people   \n",
       "5                crime, city, new, analysis, human, model, terrorist, attack, police, pattern   \n",
       "6                crime, city, new, analysis, human, model, terrorist, attack, police, pattern   \n",
       "7                crime, city, new, analysis, human, model, terrorist, attack, police, pattern   \n",
       "8                crime, city, new, analysis, human, model, terrorist, attack, police, pattern   \n",
       "9       video, time, event, surveillance, real, abnormal, algorithm, camera, criminal, people   \n",
       "\n",
       "                                                                                                                                                                                                       Text  \n",
       "0                                                                                                                                                  [security, force, likely, prediction, physical, measure]  \n",
       "1                                                                                                                         [focused, planning, prediction, practice, preparation, future, terrorist, attack]  \n",
       "2                                                                                                                                                                                       [potential, target]  \n",
       "3                                                                                                                                                                                          [recovery, plan]  \n",
       "4                                                                                                                            [statistical, forecasting, destination, specific, antiterrorism, action, plan]  \n",
       "5                                                                                                                                                            [police, chief, great, importance, prediction]  \n",
       "6                                                                                                                                           [prediction, feasible, terrorist, attack, tourism, destination]  \n",
       "7                                                                                                                      [prediction, feasible, category, terrorism, prevention, practice, tourism, director]  \n",
       "8  [potential, issue, real, time, effective, emergency, man, agement, public, safety, general, quality, life, example, critical, event, earthquake, mob, gathering, protest, spike, social, medium, volume]  \n",
       "9                                                                                             [datum, mining, diverse, real, time, social, stream, real, world, event, enable, official, vast, information]  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,6), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)  #<------- Dominant topic DATA FRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The most representative sentence for each topic\n",
    "### Sometimes you want to get samples of sentences that most represent a given topic. This code gets the most exemplar sentence for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Representative Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.947589</td>\n",
       "      <td>security, different, behavior, approach, bollard, human, service, location, measure, point</td>\n",
       "      <td>[iot, analytic, control, future, sensor, actuator, communication, connectivity, analytic, security, smart, application, web, mobile, interface]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973066</td>\n",
       "      <td>crime, city, new, analysis, human, model, terrorist, attack, police, pattern</td>\n",
       "      <td>[early, attempt, meaningful, pattern, suicide, attack, network, model, new, evolution, stimulating, lasso, absolute, shrinkage, selection, operator, logistic, regression, esallor, feature, similarity, function, important, key, important, feature, successful, interactive, terrorist, event]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.979509</td>\n",
       "      <td>detection, target, frame, large, solution, model, resource, object, value, building</td>\n",
       "      <td>[complex, iot, security, domain, iot, cybersecurity, challenge, significant, research, attention, cybersecurity, manufacturing, physical, cps, babiceanu, seker, comprehensive, end, end, standard, bersecurity, solution, smart, building, minoli, sohraby, occhiogrosso, cybersecurity, privacy, smart, home, lin, bergmann, cybersecurity, marketing, wakenshaw, cybersecurity, complex, physical, network]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.955987</td>\n",
       "      <td>vehicle, motion, device, machine, time, intelligence, space, high, layer, image</td>\n",
       "      <td>[tracking, cf, method, deep, learning, method, dcfnet, siamfc, cfnet, tracker, deep, learning, method, correla, tion, convolutional, neural, network]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.953498</td>\n",
       "      <td>video, time, event, surveillance, real, abnormal, algorithm, camera, criminal, people</td>\n",
       "      <td>[dss, predictive, policing, capability, distribution, crime, risk, territory, optimization, exploit, information, agent, possible, way, preference, decision, maker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.963662</td>\n",
       "      <td>information, edge, technology, management, sensor, dynamic, source, network, task, people</td>\n",
       "      <td>[critical, antecedent, smart, government, gov, ernment, capability, digital, technology, ample, web, iot, sensing, change, environment, public, community, contingent, demand, situation, agile, manner]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0            0.947589   \n",
       "1        1.0            0.973066   \n",
       "2        2.0            0.979509   \n",
       "3        3.0            0.955987   \n",
       "4        4.0            0.953498   \n",
       "5        5.0            0.963662   \n",
       "\n",
       "                                                                                     Keywords  \\\n",
       "0  security, different, behavior, approach, bollard, human, service, location, measure, point   \n",
       "1                crime, city, new, analysis, human, model, terrorist, attack, police, pattern   \n",
       "2         detection, target, frame, large, solution, model, resource, object, value, building   \n",
       "3             vehicle, motion, device, machine, time, intelligence, space, high, layer, image   \n",
       "4       video, time, event, surveillance, real, abnormal, algorithm, camera, criminal, people   \n",
       "5   information, edge, technology, management, sensor, dynamic, source, network, task, people   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                              Representative Text  \n",
       "0                                                                                                                                                                                                                                                                 [iot, analytic, control, future, sensor, actuator, communication, connectivity, analytic, security, smart, application, web, mobile, interface]  \n",
       "1                                                                                                               [early, attempt, meaningful, pattern, suicide, attack, network, model, new, evolution, stimulating, lasso, absolute, shrinkage, selection, operator, logistic, regression, esallor, feature, similarity, function, important, key, important, feature, successful, interactive, terrorist, event]  \n",
       "2  [complex, iot, security, domain, iot, cybersecurity, challenge, significant, research, attention, cybersecurity, manufacturing, physical, cps, babiceanu, seker, comprehensive, end, end, standard, bersecurity, solution, smart, building, minoli, sohraby, occhiogrosso, cybersecurity, privacy, smart, home, lin, bergmann, cybersecurity, marketing, wakenshaw, cybersecurity, complex, physical, network]  \n",
       "3                                                                                                                                                                                                                                                           [tracking, cf, method, deep, learning, method, dcfnet, siamfc, cfnet, tracker, deep, learning, method, correla, tion, convolutional, neural, network]  \n",
       "4                                                                                                                                                                                                                                            [dss, predictive, policing, capability, distribution, crime, risk, territory, optimization, exploit, information, agent, possible, way, preference, decision, maker]  \n",
       "5                                                                                                                                                                                                        [critical, antecedent, smart, government, gov, ernment, capability, digital, technology, ample, web, iot, sensing, change, environment, public, community, contingent, demand, situation, agile, manner]  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(9) #<------- Most Representative sentence in each topic DATA FRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution of Word Counts in Documents\n",
    "### When working with a large number of documents, you want to know how big the documents are as a whole\n",
    "### and by topic. Let’s plot the document word counts distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(100, 17.0, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(100,  14.0, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(100,  11.0, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(100,  8.0, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(100,  5.0, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 100), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,115,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=15))\n",
    "plt.show()\n",
    "#plt.savefig(\"C:/Users/20185123/Desktop/distribution_document_word_count.png\") # save as png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Word Counts in Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(3,2,figsize=(16,14), dpi=160, sharex=True, sharey=True) ## to show multiple topics modify the size\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins = 100, color=cols[i])\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "    ax.set(xlim=(0, 100), xlabel='Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i], size=12)\n",
    "    ax.set_title('Topic: '+str(i+1), fontdict=dict(size=12, color=cols[i]))### str(i+1) renames the topics starting from 1 instead of 0\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,110,9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=20)\n",
    "plt.show()\n",
    "#plt.savefig(\"C:/Users/20185123/Desktop/distribution_word_count_in_doc.png\") # save as png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds of Top N Keywords in Each Topic\n",
    "### a word cloud with the size of the words proportional to the weight is a pleasant sight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(8,8), sharex=True, sharey=True) ## to show multiple topics modify the size\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i+1), fontdict=dict(size=20))### str(i+1) renames the topics starting from 1 instead of 0\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig(\"C:/Users/20185123/Desktop/word_cloud_per_topic2.png\") # save as png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counts of Topic Keywords\n",
    "### how frequently the words have appeared in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.045); ax.set_ylim(0, 7000)\n",
    "    ax.set_title('Topic: ' + str(i+1), color=cols[i], fontsize=16)### str(i+1) renames the topics starting from 1 instead of 0\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()\n",
    "#plt.savefig(\"C:/Users/20185123/Desktop/word_count_importance_of_topic_keywords.png\") # save as png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Chart Colored by Topic\n",
    "### Each word in the document is representative of one of the 4 topics. \n",
    "### Let’s color each word in the given documents by the topic id it is attributed to.\n",
    "### The color of the enclosing rectangle is the topic assigned to the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
    "                    fontsize=15, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=400)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=13, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=20, y=0.95, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #plt.savefig(\"C:/Users/20185123/Desktop/sentence_topic_coloring.png\") # save as png\n",
    "    \n",
    "sentences_chart()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the most discussed topics in the documents?\n",
    "### Let’s compute the total number of documents attributed to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Sentences\n",
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
    "\n",
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 4] ### chose the number of words under the topic\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let’s make two plots:\n",
    "\n",
    "### 1. The number of documents for each topic by assigning the document to the topic that has the most weight in that document.\n",
    "### 2. The number of documents for each topic by by summing up the actual weight contribution of each topic to respective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), dpi=120, sharey=True)\n",
    "\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x+1)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0]) ### str(x+1) in order to start counting from 1 instead of 0\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "ax1.set_ylabel('Number of Documents')\n",
    "ax1.set_ylim(0, 360)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig(\"C:/Users/20185123/Desktop/N_doc_topic_and_weight.png\") # save as png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE Clustering Chart\n",
    "### Let’s visualize the clusters of documents in a 2D space using t-SNE (t-distributed stochastic neighbor embedding) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook, export_png\n",
    "\n",
    "from bokeh.embed import components, file_html\n",
    "from bokeh.resources import CDN\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "    \n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=500, angle=.99, metric='euclidean', init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 6\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "\n",
    "show(plot)\n",
    "#plt.savefig(\"C:/Users/20185123/Desktop/t-SNE_Clustering.png\") # save as png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Analysis\n",
    "### Using kmapper to create a 2D topology based on the word2vec extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim, logging\n",
    "\n",
    "# df ##whole dataset\n",
    "# data_words ##dataset in a list\n",
    "# data_ready ##dataset lemmatized\n",
    "\n",
    "model = Word2Vec(data_ready)\n",
    "X = model[model.wv.vocab]\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "# Import the class\n",
    "import kmapper as km\n",
    "from sklearn import cluster\n",
    "\n",
    "# Initialize\n",
    "mapper = km.KeplerMapper(verbose=1)\n",
    "\n",
    "# Fit to and transform the data\n",
    "projected_data = mapper.fit_transform(X, projection=[0,1]) # X-Y axis\n",
    "\n",
    "# Create dictionary called 'graph' with nodes, edges and meta-information\n",
    "graph = mapper.map(projected_data, X, clusterer=cluster.AgglomerativeClustering(n_clusters=6,\n",
    "                                                             linkage=\"complete\",\n",
    "                                                             affinity=\"cosine\"),\n",
    "                   overlap_perc=0.2)\n",
    "\n",
    "# Visualize it\n",
    "mapper.visualize(graph, path_html=\"visualization/word2vec_analysis/prova.html\", X =X, X_names = words,\n",
    "                 title=\"Topological analysis word2vec\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset for Topological analisys - list of list dataset\n",
    "\n",
    "# import csv\n",
    "\n",
    "# #df = pd.read_csv('protect_dataset_nodata.csv', sep=',', encoding='latin-1', engine='python')\n",
    "\n",
    "# with open('CodingOverview_topol_analysis.csv', newline='') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     topological_analysis = list(reader)\n",
    "    \n",
    "# print(topological_analysis[:4])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Topological Data Anlysis Mapper with LSA \n",
    "### https://kepler-mapper.scikit-tda.org/notebooks/KeplerMapper-Newsgroup20-Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kmapper import jupyter\n",
    "import kmapper as km\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import cluster\n",
    "\n",
    "# df ##whole dataset\n",
    "# data_words ##dataset in a list\n",
    "# data_ready ##dataset lemmatized\n",
    "\n",
    "X = np.array([string for elem in top_analysis for string in elem])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "##### max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "##### max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "##### The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "###################################################################################\n",
    "##### min_df is used for removing terms that appear too infrequently. For example:\n",
    "##### min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "##### min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "##### The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n",
    "###################################################################################\n",
    "##### ngram_range : tuple (min_n, max_n)\n",
    "##### The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "projected_X = mapper.fit_transform(X,\n",
    "    projection=[TfidfVectorizer(analyzer=\"char\",\n",
    "                                ngram_range=(1,9),\n",
    "                                max_df=0.95,\n",
    "                                min_df=0.03\n",
    "                               ),\n",
    "                TruncatedSVD(n_components=100,\n",
    "                             random_state=1729),\n",
    "                Isomap(n_components=2,\n",
    "                       n_jobs=-1)],\n",
    "    scaler=[None, None, MinMaxScaler()])\n",
    "\n",
    "print(projected_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = mapper.map(projected_X,\n",
    "                   clusterer=cluster.AgglomerativeClustering(n_clusters=6,\n",
    "                                                             linkage=\"complete\",\n",
    "                                                             affinity=\"cosine\"),\n",
    "                   overlap_perc=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                      strip_accents=\"unicode\",\n",
    "                      stop_words=\"english\",\n",
    "                      ngram_range=(1,3),\n",
    "                      max_df=0.99,\n",
    "                      min_df=0.01)\n",
    "\n",
    "interpretable_inverse_X = vec.fit_transform(X).toarray()\n",
    "interpretable_inverse_X_names = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = mapper.visualize(graph, \n",
    "                        path_html=\"visualization/topological_data_analysis/prova.html\",\n",
    "                        X=interpretable_inverse_X,\n",
    "                        X_names=interpretable_inverse_X_names,\n",
    "                        title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
